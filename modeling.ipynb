{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d54919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Setup\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, random, math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import EsmTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import torch\n",
    "\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "MASK_ID = tokenizer.mask_token_id\n",
    "PAD_ID  = tokenizer.pad_token_id\n",
    "BOS_ID  = tokenizer.bos_token_id\n",
    "EOS_ID  = tokenizer.eos_token_id\n",
    "\n",
    "# canonical AAs for *sampling* later (don’t clamp during training CE)\n",
    "CANON = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "CANON_IDS = tokenizer.convert_tokens_to_ids(CANON)\n",
    "\n",
    "# --- 1) Data\n",
    "ds = load_dataset(\"dahvid12/uniprot50-sequences-subsample100\", split=\"train\")\n",
    "seq_series = ds.to_pandas()[\"sequence\"]  # pandas Series[str]\n",
    "\n",
    "\n",
    "class ProteinSeqDataset(Dataset):\n",
    "    def __init__(self, s):\n",
    "        self.items = [str(x).strip().upper() for x in s.tolist()]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "def collate_batch(batch_seqs, max_len=None):\n",
    "    toks = tokenizer(\n",
    "        batch_seqs,\n",
    "        add_special_tokens=True, padding=True, truncation=True, max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = toks[\"input_ids\"]          # [B, Lw]\n",
    "    attn      = toks[\"attention_mask\"]     # [B, Lw]\n",
    "    core_ids  = input_ids[:, 1:-1].contiguous()      # drop BOS/EOS\n",
    "    core_attn = attn[:, 1:-1].contiguous()\n",
    "    # ensure PAD_ID in padded spots (robust)\n",
    "    core_ids  = torch.where(core_attn.bool(), core_ids, torch.full_like(core_ids, PAD_ID))\n",
    "    return {\"core_ids\": core_ids, \"core_attn\": core_attn}\n",
    "\n",
    "# --- 2) MDLM schedules (cosine α) and weight\n",
    "def alpha_cosine(t: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.cos(0.5 * math.pi * t).pow(2)\n",
    "\n",
    "def alpha_prime_cosine(t: torch.Tensor) -> torch.Tensor:\n",
    "    a = 0.5 * math.pi\n",
    "    return -2*a*torch.cos(a*t)*torch.sin(a*t)\n",
    "\n",
    "def weight_w(t: torch.Tensor) -> torch.Tensor:\n",
    "    a = alpha_cosine(t)\n",
    "    ap = alpha_prime_cosine(t)\n",
    "    return ap / (1.0 - a).clamp_min(1e-6)\n",
    "\n",
    "# --- 3) Forward masking that respects padding\n",
    "def forward_mask(core_ids: torch.LongTensor, core_attn: torch.LongTensor, t_scalar: float):\n",
    "    \"\"\"\n",
    "    Mask only where core_attn==1. Returns z_t, masked_bool.\n",
    "    \"\"\"\n",
    "    B, L = core_ids.shape\n",
    "    valid = core_attn.bool()\n",
    "    a = alpha_cosine(torch.tensor([t_scalar], device=core_ids.device))\n",
    "    p_mask = (1.0 - a).item()\n",
    "    U = torch.rand(B, L, device=core_ids.device)\n",
    "    masked = (U < p_mask) & valid\n",
    "    z_t = core_ids.clone()\n",
    "    z_t[masked] = MASK_ID\n",
    "    return z_t, masked\n",
    "\n",
    "# --- 4) Simple time-conditioned encoder (same signature as earlier)\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, d): \n",
    "        super().__init__()\n",
    "        self.lin = nn.Sequential(nn.Linear(d, 4*d), nn.SiLU(), nn.Linear(4*d, d))\n",
    "        self.d = d\n",
    "    def forward(self, t_scalar, device):\n",
    "        t = torch.tensor([t_scalar], device=device).float()\n",
    "        half = self.d // 2\n",
    "        freqs = torch.exp(-math.log(10000.0) * torch.arange(half, device=device)/half)\n",
    "        ang = t[:,None]*freqs[None,:]\n",
    "        te = torch.cat([ang.sin(), ang.cos()], dim=-1)\n",
    "        return self.lin(te)\n",
    "\n",
    "class MDLMTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_layers=12, n_heads=8, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d = d_model\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "        enc = nn.TransformerEncoderLayer(d_model, n_heads, 4*d_model, dropout=dropout,\n",
    "                                         batch_first=True, norm_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.emb.weight   # tie\n",
    "        self.time = TimeEmbed(d_model)\n",
    "    def forward(self, core_ids, t_scalar):\n",
    "        B, L = core_ids.shape\n",
    "        h = self.emb(core_ids) + self.pos.weight[:L][None].expand(B, L, -1) + self.time(t_scalar, core_ids.device)\n",
    "        h = self.tr(h)\n",
    "        return self.head(h)                  # [B,L,V]\n",
    "\n",
    "# --- 5) MDLM loss (Algorithm 1) that ignores PADs\n",
    "def mdlm_loss_step(model, core_ids, core_attn, t_scalar: float, return_metrics: bool = False):\n",
    "    \"\"\"\n",
    "    MDLM loss per Algorithm 1 with weight w(t) = alpha'(t)/(1-alpha(t)).\n",
    "    Returns loss (and optional metrics dict).\n",
    "    \"\"\"\n",
    "    z_t, masked = forward_mask(core_ids, core_attn, t_scalar)\n",
    "    logits = model(z_t, t_scalar)           # [B,L,V]\n",
    "\n",
    "    # Count tokens\n",
    "    valid_tokens  = core_attn.sum().item()\n",
    "    masked_tokens = masked.sum().item()\n",
    "    masked_frac   = masked_tokens / max(1, valid_tokens)\n",
    "\n",
    "    if masked_tokens == 0:\n",
    "        loss = logits.new_tensor(0.0, requires_grad=True)\n",
    "    else:\n",
    "        targets  = core_ids[masked]         # [Nmask]\n",
    "        logits_m = logits[masked]           # [Nmask,V]\n",
    "        ce = torch.nn.functional.cross_entropy(logits_m, targets, reduction=\"mean\")\n",
    "        w  = weight_w(torch.tensor([t_scalar], device=logits.device)).item()\n",
    "        loss = ce * float(w)\n",
    "\n",
    "    if not return_metrics:\n",
    "        return loss\n",
    "\n",
    "    # metrics payload\n",
    "    a_t = alpha_cosine(torch.tensor([t_scalar], device=core_ids.device)).item()\n",
    "    ap_t = alpha_prime_cosine(torch.tensor([t_scalar], device=core_ids.device)).item()\n",
    "    return loss, {\n",
    "        \"t\": t_scalar,\n",
    "        \"alpha_t\": a_t,\n",
    "        \"alpha_prime_t\": ap_t,\n",
    "        \"w_t\": ap_t / max(1e-6, 1 - a_t),\n",
    "        \"valid_tokens\": int(valid_tokens),\n",
    "        \"masked_tokens\": int(masked_tokens),\n",
    "        \"masked_frac\": float(masked_frac),\n",
    "        \"mean_len\": float(core_attn.sum(dim=1).float().mean().item()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f151be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (B) Nice-to-have: gradient-norm + CUDA mem helpers ---\n",
    "def grad_global_norm(module: torch.nn.Module) -> float:\n",
    "    total = 0.0\n",
    "    for p in module.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total += param_norm.item() ** 2\n",
    "    return total ** 0.5\n",
    "\n",
    "def cuda_mem_mb():\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0.0, 0.0\n",
    "    alloc = torch.cuda.memory_allocated() / (1024**2)\n",
    "    reserv = torch.cuda.memory_reserved() / (1024**2)\n",
    "    return alloc, reserv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33900998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ProteinSeqDataset(seq_series),\n",
    "    batch_size=32, shuffle=True, num_workers=2, pin_memory=True,\n",
    "    collate_fn=lambda b: collate_batch(b, max_len=1024),  # ESM limit ≈1024 incl. specials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf24519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/protDiffuse/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- (C) Train with tqdm + live metrics ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MDLMTransformer(vocab_size=tokenizer.vocab_size, d_model=512, n_layers=12, n_heads=8).to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/5:   1%|          | 158/21433 [03:37<6:33:46,  1.11s/it, loss=-85856377.5372, mask%=83.1, len=224, α(t)=0.171, w(t)=-1.427, g||=1.00, tok/s=6267, CUDA(MB)=657/20412] "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model.train()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_tokens = 0\n",
    "    epoch_masked = 0\n",
    "    epoch_loss_sum = 0.0\n",
    "    epoch_steps = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"epoch {epoch}/{epochs}\", leave=True)\n",
    "    for batch in pbar:\n",
    "        core_ids  = batch[\"core_ids\"].to(device, non_blocking=True)\n",
    "        core_attn = batch[\"core_attn\"].to(device, non_blocking=True)\n",
    "\n",
    "        t = random.random()  # t ~ U[0,1]\n",
    "        loss, m = mdlm_loss_step(model, core_ids, core_attn, t, return_metrics=True)\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "\n",
    "        # accounting\n",
    "        epoch_steps += 1\n",
    "        epoch_loss_sum += float(loss.detach().item())\n",
    "        epoch_tokens += m[\"valid_tokens\"]\n",
    "        epoch_masked += m[\"masked_tokens\"]\n",
    "\n",
    "        # live stats\n",
    "        gn = grad_global_norm(model)\n",
    "        alloc, reserv = cuda_mem_mb()\n",
    "        elapsed = max(1e-6, time.time() - t0)\n",
    "        toks_per_s = epoch_tokens / elapsed\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{(epoch_loss_sum/epoch_steps):.4f}\",\n",
    "            \"mask%\": f\"{(100.0 * m['masked_frac']):.1f}\",\n",
    "            \"len\": f\"{m['mean_len']:.0f}\",\n",
    "            \"α(t)\": f\"{m['alpha_t']:.3f}\",\n",
    "            \"w(t)\": f\"{m['w_t']:.3f}\",\n",
    "            \"g||\": f\"{gn:.2f}\",\n",
    "            \"tok/s\": f\"{toks_per_s:.0f}\",\n",
    "            \"CUDA(MB)\": f\"{alloc:.0f}/{reserv:.0f}\" if torch.cuda.is_available() else \"CPU\",\n",
    "        })\n",
    "\n",
    "    epoch_time = time.time() - t0\n",
    "    print(\n",
    "        f\"epoch {epoch}: \"\n",
    "        f\"loss={epoch_loss_sum/epoch_steps:.4f} | \"\n",
    "        f\"masked={epoch_masked}/{epoch_tokens} ({100*epoch_masked/max(1,epoch_tokens):.1f}%) | \"\n",
    "        f\"tok/s={epoch_tokens/max(1e-6, epoch_time):.0f} | \"\n",
    "        f\"time={epoch_time:.1f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc50182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protDiffuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
